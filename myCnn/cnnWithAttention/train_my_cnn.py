import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from IPython import get_ipython
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
import os
import sys
import datetime
from IPython.display import display, clear_output  # æ·»åŠ clear_outputç”¨äºå®æ—¶æ›´æ–°
# æ˜¾ç¤ºä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


# --- ç»˜å›¾åŠŸèƒ½å‡½æ•° ---

def setup_plotting() :
    """åˆå§‹åŒ–matplotlibå›¾å½¢å’Œåæ ‡è½´"""
    # è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒ
    if 'ipykernel' in sys.modules :
        get_ipython().run_line_magic('matplotlib', 'inline')
        plt.ion()
    else :
        plt.ioff()

    fig, axes = plt.subplots(3, 1, figsize=(10, 12))
    ax_loss, ax_top1, ax_top3 = axes

    # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
    metrics = {
        'epochs' : [],
        'train_loss' : [], 'val_loss' : [],
        'train_top1' : [], 'val_top1' : [],
        'train_top3' : [], 'val_top3' : []
    }

    # åˆ›å»ºç»˜å›¾çº¿æ¡
    lines = {
        'train_loss' : ax_loss.plot([], [], label='Train Loss', color='tab:blue', marker='o')[0],
        'val_loss' : ax_loss.plot([], [], label='Val Loss', color='tab:orange', marker='s')[0],
        'train_top1' : ax_top1.plot([], [], label='Train Top-1', color='tab:blue', marker='o')[0],
        'val_top1' : ax_top1.plot([], [], label='Val Top-1', color='tab:orange', marker='s')[0],
        'train_top3' : ax_top3.plot([], [], label='Train Top-3', color='tab:blue', marker='o')[0],
        'val_top3' : ax_top3.plot([], [], label='Val Top-3', color='tab:orange', marker='s')[0]
    }

    # é…ç½®å›¾è¡¨
    ax_loss.set_title('Training & Validation Loss')
    ax_loss.set_ylabel('Loss')
    ax_loss.legend()

    ax_top1.set_title('Top-1 Accuracy')
    ax_top1.set_ylabel('Accuracy (%)')
    ax_top1.set_ylim(0, 1)
    ax_top1.legend()

    ax_top3.set_title('Top-3 Accuracy')
    ax_top3.set_ylabel('Accuracy (%)')
    ax_top3.set_xlabel('Epoch')
    ax_top3.set_ylim(0, 1)
    ax_top3.legend()

    plt.tight_layout()
    return fig, axes, lines, metrics


def update_plot(fig, axes, lines, metrics, epoch,
                train_loss, val_loss, train_top1, val_top1, train_top3, val_top3) :
    """æ›´æ–°ç»˜å›¾æ•°æ®"""
    # æ›´æ–°æ•°æ®
    metrics['epochs'].append(epoch + 1)
    metrics['train_loss'].append(train_loss)
    metrics['val_loss'].append(val_loss)
    metrics['train_top1'].append(train_top1)
    metrics['val_top1'].append(val_top1)
    metrics['train_top3'].append(train_top3)
    metrics['val_top3'].append(val_top3)

    # æ›´æ–°çº¿æ¡
    for key in lines :
        lines[key].set_data(metrics['epochs'], metrics[key])

    # è°ƒæ•´åæ ‡è½´
    for ax in axes :
        ax.relim()
        ax.autoscale_view()






def save_plot(fig, save_dir="training_plots") :
    """ä¿å­˜æœ€ç»ˆå›¾è¡¨"""
    display(fig)
    os.makedirs(save_dir, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(save_dir, f'training_metrics_{timestamp}.png')
    fig.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    plt.close(fig)


# --- ç‰¹å¾å›¾å¯è§†åŒ– ---

class FeatureMapVisualizer :
    def __init__(self, model, layer_name=None) :
        self.model = model
        self.feature_maps = []
        self.hook_handle = None
        self.setup_hook(layer_name)

    def setup_hook(self, layer_name=None) :
        """è‡ªåŠ¨æˆ–æ‰‹åŠ¨è®¾ç½®ç‰¹å¾å›¾é’©å­"""
        # æŸ¥æ‰¾ç›®æ ‡å±‚
        target_layer = None
        for name, module in self.model.named_modules() :
            if isinstance(module, nn.Conv2d) :
                if layer_name is None or name == layer_name :
                    target_layer = module
                    break

        if target_layer is None :
            raise ValueError("æœªæ‰¾åˆ°åˆé€‚çš„å·ç§¯å±‚")

        # æ³¨å†Œé’©å­
        def hook_fn(module, input, output) :
            if output.dim() == 4 :  # [batch, channel, height, width]
                feat_map = output[0].detach().cpu()  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                self.feature_maps.append(feat_map.mean(dim=0))  # é€šé“å¹³å‡

        self.hook_handle = target_layer.register_forward_hook(hook_fn)
        print(f"ğŸ” ç‰¹å¾å›¾é’©å­å·²æ³¨å†Œåˆ°: {target_layer}")

    def visualize(self, epoch, log_dir=None) :
        """å¯è§†åŒ–æœ€æ–°ç‰¹å¾å›¾"""
        if not self.feature_maps :
            print("âš ï¸ æ— ç‰¹å¾å›¾æ•°æ®")
            return

        feat_map = self.feature_maps[-1].numpy()
        feat_map = (feat_map - feat_map.min()) / (feat_map.max() - feat_map.min() + 1e-8)

        fig, ax = plt.subplots(1, 2, figsize=(12, 5))

        # åŸå§‹ç‰¹å¾å›¾
        im0 = ax[0].imshow(feat_map, cmap='viridis')
        ax[0].set_title(f"Epoch {epoch} - åŸå§‹ç‰¹å¾å›¾")
        plt.colorbar(im0, ax=ax[0])

        # çƒ­åŠ›å›¾
        im1 = ax[1].imshow(feat_map, cmap='hot')
        ax[1].set_title(f"Epoch {epoch} - çƒ­åŠ›å›¾")
        plt.colorbar(im1, ax=ax[1])

        plt.tight_layout()

        # æ˜¾ç¤ºå’Œä¿å­˜
        if 'ipykernel' in sys.modules :
            display(fig)

        if log_dir :
            os.makedirs(log_dir, exist_ok=True)
            save_path = os.path.join(log_dir, f'feature_map_epoch_{epoch}.png')
            fig.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ–¼ï¸ ç‰¹å¾å›¾å·²ä¿å­˜åˆ°: {save_path}")

        # ç§»é™¤ plt.close(fig)ï¼Œç¡®ä¿å›¾è¡¨ä¿æŒæ‰“å¼€çŠ¶æ€
        # plt.close(fig)
        # self.feature_maps.clear()  # æ¸…ç©ºç¼“å­˜

    def remove_hook(self) :
        """ç§»é™¤é’©å­"""
        if self.hook_handle is not None :
            self.hook_handle.remove()
            print("âœ… ç‰¹å¾å›¾é’©å­å·²ç§»é™¤")
        else :
            print("âš ï¸ æ²¡æœ‰å¯ç§»é™¤çš„é’©å­")



# --- ä¸»è®­ç»ƒå‡½æ•° ---

def train_and_validate(model, train_loader, val_loader, epochs, device='cpu',
                       save_path='best_model.pth', save_best_only=True,
                       patience=10, lr=1e-4, auto_lr=False,
                       log_dir=None, plot_results=True) :
    """å®Œæ•´çš„è®­ç»ƒå’ŒéªŒè¯æµç¨‹"""

    # åˆå§‹åŒ–
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # æ—¥å¿—è®°å½•
    if log_dir is None :
        log_dir = f"runs/{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    writer = SummaryWriter(log_dir=log_dir)

    # ç‰¹å¾å›¾å¯è§†åŒ–
    fm_visualizer = FeatureMapVisualizer(model)

    # ç»˜å›¾åˆå§‹åŒ–
    if plot_results :
        fig, axes, lines, metrics = setup_plotting()

    # è®­ç»ƒå¾ªç¯
    best_val_acc = 0.0
    no_improve = 0

    for epoch in range(epochs) :
        model.train()
        train_loss, train_top1, train_top3 = 0.0, 0.0, 0.0
        total = 0

        # è®­ç»ƒé˜¶æ®µ
        for inputs, targets in train_loader :
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            loss.backward()
            optimizer.step()

            # è®¡ç®—æŒ‡æ ‡
            train_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_top1 += (preds == targets).sum().item()

            _, top3_preds = torch.topk(outputs, 3, dim=1)
            train_top3 += torch.sum(top3_preds == targets.view(-1, 1)).item()

            total += targets.size(0)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_top1, val_top3 = 0.0, 0.0, 0.0
        val_total = 0

        model.eval()
        with torch.no_grad() :
            for inputs, targets in val_loader :
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                val_top1 += (preds == targets).sum().item()

                _, top3_preds = torch.topk(outputs, 3, dim=1)
                val_top3 += torch.sum(top3_preds == targets.view(-1, 1)).item()

                val_total += targets.size(0)

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        train_loss /= total
        train_top1 /= total
        train_top3 /= total
        val_loss /= val_total
        val_top1 /= val_total
        val_top3 /= val_total

        # æ›´æ–°å›¾è¡¨
        if plot_results :
            update_plot(fig, axes, lines, metrics, epoch,
                        train_loss, val_loss, train_top1, val_top1, train_top3, val_top3)

        # ç‰¹å¾å›¾å¯è§†åŒ–
        if epoch % 10 == 0 :  # æ¯5ä¸ªepochå¯è§†åŒ–ä¸€æ¬¡
            fm_visualizer.visualize(epoch, log_dir)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_top1 > best_val_acc :
            best_val_acc = val_top1
            no_improve = 0
            if save_best_only :
                torch.save(model.state_dict(), save_path)
                print(f"ğŸ¯ æ–°æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {val_top1:.4f}) å·²ä¿å­˜åˆ° {save_path}")
        else :
            no_improve += 1

        # æ—©åœæ£€æŸ¥
        if no_improve >= patience :
            print(f"â¹ï¸ æ—©åœè§¦å‘: éªŒè¯å‡†ç¡®ç‡è¿ç»­ {patience} è½®æœªæå‡")
            break

        # æ‰“å°æ—¥å¿—
        print(f"Epoch {epoch + 1}/{epochs}: "
              f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
              f"Train Top-1: {train_top1:.4f} | Val Top-1: {val_top1:.4f} | "
              f"Train Top-3: {train_top3:.4f} | Val Top-3: {val_top3:.4f}")

    # æ¸…ç†
    fm_visualizer.remove_hook()
    writer.close()

    if plot_results :
        save_plot(fig)

    return model

