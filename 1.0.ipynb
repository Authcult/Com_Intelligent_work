{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于卷积神经网络的手写英文字母识别系统研究",
   "id": "114cf15279a6d9fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 准备数据集及数据预处理",
   "id": "2c0575d839810a6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 将下载的数据集按类重命名",
   "id": "c7a32a907d5976a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:24.731215Z",
     "start_time": "2025-04-20T03:09:24.724947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import string\n",
    "\n",
    "# 定义源目录路径\n",
    "# source_dir = \"EnglishImg/English/Img/GoodImg/Bmp\"\n",
    "source_dir = \"EnglishHnd/English/Hnd/Img\"\n",
    "\n",
    "# 生成目标文件夹名称列表\n",
    "target_folders = list(string.digits) + list(string.ascii_uppercase) + [f\"{char}_\" for char in string.ascii_lowercase]\n",
    "\n",
    "# 获取源目录下的所有文件夹名称\n",
    "source_folders = sorted([f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))])\n",
    "\n",
    "# 确保源文件夹数量与目标文件夹数量一致\n",
    "if len(source_folders) != len(target_folders):\n",
    "    raise ValueError(\"源文件夹数量与目标文件夹数量不一致\")\n",
    "\n",
    "# 重命名文件夹\n",
    "for source_folder, target_folder in zip(source_folders, target_folders):\n",
    "    source_path = os.path.join(source_dir, source_folder)\n",
    "    target_path = os.path.join(source_dir, target_folder)\n",
    "\n",
    "    try:\n",
    "        os.rename(source_path, target_path)\n",
    "        print(f\"重命名: {source_path} -> {target_path}\")\n",
    "    except FileExistsError:\n",
    "        print(f\"目标文件夹 {target_path} 已存在，跳过重命名 {source_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"重命名 {source_path} 到 {target_path} 时出错: {e}\")\n",
    "\"\"\"\n"
   ],
   "id": "981cf30f665a69a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport string\\n\\n# 定义源目录路径\\n# source_dir = \"EnglishImg/English/Img/GoodImg/Bmp\"\\nsource_dir = \"EnglishHnd/English/Hnd/Img\"\\n\\n# 生成目标文件夹名称列表\\ntarget_folders = list(string.digits) + list(string.ascii_uppercase) + [f\"{char}_\" for char in string.ascii_lowercase]\\n\\n# 获取源目录下的所有文件夹名称\\nsource_folders = sorted([f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))])\\n\\n# 确保源文件夹数量与目标文件夹数量一致\\nif len(source_folders) != len(target_folders):\\n    raise ValueError(\"源文件夹数量与目标文件夹数量不一致\")\\n\\n# 重命名文件夹\\nfor source_folder, target_folder in zip(source_folders, target_folders):\\n    source_path = os.path.join(source_dir, source_folder)\\n    target_path = os.path.join(source_dir, target_folder)\\n\\n    try:\\n        os.rename(source_path, target_path)\\n        print(f\"重命名: {source_path} -> {target_path}\")\\n    except FileExistsError:\\n        print(f\"目标文件夹 {target_path} 已存在，跳过重命名 {source_path}\")\\n    except Exception as e:\\n        print(f\"重命名 {source_path} 到 {target_path} 时出错: {e}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:24.779641Z",
     "start_time": "2025-04-20T03:09:24.773408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "from torchvision.datasets import EMNIST\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 加载 EMNIST ByClass 子集（训练+测试）\n",
    "train_set = EMNIST(root=\"emnist_data\", split=\"byclass\", train=True, download=True)\n",
    "test_set = EMNIST(root=\"emnist_data\", split=\"byclass\", train=False, download=True)\n",
    "\n",
    "# 合并训练和测试数据\n",
    "full_dataset = train_set + test_set\n",
    "\n",
    "# 输出图像保存目录\n",
    "save_dir = \"emnist_png_byclass\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 编号 0~61 映射到字符标签（0-9, A-Z, a-z）\n",
    "def label_to_char(label):\n",
    "    if 0 <= label <= 9:\n",
    "        return str(label)  # 数字 0-9\n",
    "    elif 10 <= label <= 35:\n",
    "        return chr(ord('A') + label - 10)  # 大写字母 A-Z\n",
    "    elif 36 <= label <= 61:\n",
    "        return chr(ord('a') + label - 36)  # 小写字母 a-z\n",
    "    else:\n",
    "        return \"UNK\"  # 不合法\n",
    "\n",
    "# 每个类别最多保存 300 张图片\n",
    "max_images_per_class = 300\n",
    "images_per_class = {chr(i): 0 for i in range(48, 58)}  # 数字\n",
    "images_per_class.update({chr(i): 0 for i in range(65, 91)})  # 大写字母\n",
    "images_per_class.update({chr(i): 0 for i in range(97, 123)})  # 小写字母\n",
    "\n",
    "# 保存图像\n",
    "saved_images_count = 0\n",
    "for i, (img, label) in enumerate(full_dataset):\n",
    "    label_char = label_to_char(label)\n",
    "\n",
    "    # 如果该类别已保存 300 张，跳过\n",
    "    if images_per_class[label_char] >= max_images_per_class:\n",
    "        continue\n",
    "\n",
    "    # 判断大写还是小写字母，并根据前缀创建目录\n",
    "    if 'A' <= label_char <= 'Z':  # 大写字母\n",
    "        label_dir = os.path.join(save_dir, \"uppercase\", label_char)\n",
    "    elif 'a' <= label_char <= 'z':  # 小写字母\n",
    "        label_dir = os.path.join(save_dir, \"lowercase\", label_char)\n",
    "    else:  # 数字\n",
    "        label_dir = os.path.join(save_dir, \"digits\", label_char)\n",
    "\n",
    "    os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "    # 将图像调整为 28x28\n",
    "    img = img.resize((28, 28))\n",
    "\n",
    "     # 转正图像方向\n",
    "    img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT).rotate(90)\n",
    "\n",
    "    # 保存图像为 PNG\n",
    "    img.save(os.path.join(label_dir, f\"{images_per_class[label_char]}.png\"))\n",
    "\n",
    "    # 更新计数\n",
    "    images_per_class[label_char] += 1\n",
    "    saved_images_count += 1\n",
    "\n",
    "    if saved_images_count >= 20000:  # 总共保存 20,000 张\n",
    "        break\n",
    "\n",
    "    if saved_images_count % 500 == 0:\n",
    "        print(f\"已保存 {saved_images_count} 张图像...\")\n",
    "\n",
    "print(\"所有图像已保存到\", save_dir)\n",
    "\"\"\"\n"
   ],
   "id": "673e1b84cf7b90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torchvision.datasets import EMNIST\\nfrom torchvision import transforms\\nfrom PIL import Image\\nimport os\\n\\n# 加载 EMNIST ByClass 子集（训练+测试）\\ntrain_set = EMNIST(root=\"emnist_data\", split=\"byclass\", train=True, download=True)\\ntest_set = EMNIST(root=\"emnist_data\", split=\"byclass\", train=False, download=True)\\n\\n# 合并训练和测试数据\\nfull_dataset = train_set + test_set\\n\\n# 输出图像保存目录\\nsave_dir = \"emnist_png_byclass\"\\nos.makedirs(save_dir, exist_ok=True)\\n\\n# 编号 0~61 映射到字符标签（0-9, A-Z, a-z）\\ndef label_to_char(label):\\n    if 0 <= label <= 9:\\n        return str(label)  # 数字 0-9\\n    elif 10 <= label <= 35:\\n        return chr(ord(\\'A\\') + label - 10)  # 大写字母 A-Z\\n    elif 36 <= label <= 61:\\n        return chr(ord(\\'a\\') + label - 36)  # 小写字母 a-z\\n    else:\\n        return \"UNK\"  # 不合法\\n\\n# 每个类别最多保存 300 张图片\\nmax_images_per_class = 300\\nimages_per_class = {chr(i): 0 for i in range(48, 58)}  # 数字\\nimages_per_class.update({chr(i): 0 for i in range(65, 91)})  # 大写字母\\nimages_per_class.update({chr(i): 0 for i in range(97, 123)})  # 小写字母\\n\\n# 保存图像\\nsaved_images_count = 0\\nfor i, (img, label) in enumerate(full_dataset):\\n    label_char = label_to_char(label)\\n\\n    # 如果该类别已保存 300 张，跳过\\n    if images_per_class[label_char] >= max_images_per_class:\\n        continue\\n\\n    # 判断大写还是小写字母，并根据前缀创建目录\\n    if \\'A\\' <= label_char <= \\'Z\\':  # 大写字母\\n        label_dir = os.path.join(save_dir, \"uppercase\", label_char)\\n    elif \\'a\\' <= label_char <= \\'z\\':  # 小写字母\\n        label_dir = os.path.join(save_dir, \"lowercase\", label_char)\\n    else:  # 数字\\n        label_dir = os.path.join(save_dir, \"digits\", label_char)\\n\\n    os.makedirs(label_dir, exist_ok=True)\\n\\n    # 将图像调整为 28x28\\n    img = img.resize((28, 28))\\n\\n     # 转正图像方向\\n    img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT).rotate(90)\\n\\n    # 保存图像为 PNG\\n    img.save(os.path.join(label_dir, f\"{images_per_class[label_char]}.png\"))\\n\\n    # 更新计数\\n    images_per_class[label_char] += 1\\n    saved_images_count += 1\\n\\n    if saved_images_count >= 20000:  # 总共保存 20,000 张\\n        break\\n\\n    if saved_images_count % 500 == 0:\\n        print(f\"已保存 {saved_images_count} 张图像...\")\\n\\nprint(\"所有图像已保存到\", save_dir)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 划分训练集和测试集",
   "id": "8abe9a2e3577e49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.340514Z",
     "start_time": "2025-04-20T03:09:24.824327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torchvision import utils\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#使用tensorboardX进行可视化\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "#数据增强\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "#参数定义\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "# ROOT_DIR = \"EnglishHND/English/Img/GoodImg/Bmp\"\n",
    "# ROOT_DIR = \"EnglishHnd/English/Hnd/Img\"\n",
    "ROOT_DIR = \"emnist_png_balanced\"\n",
    "LOG_DIR = f\"runs/handwriting_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "MODEL_SAVE_PATH = \"cnn_res_attention_best.pth\""
   ],
   "id": "3858a0e49450d6b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.355138Z",
     "start_time": "2025-04-20T03:09:35.340514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义数据预处理变换\n",
    "# transform = T.Compose([\n",
    "#     T.Resize((28, 28)), # 统一尺寸为28x28\n",
    "#     T.Grayscale(num_output_channels=1), # 灰度处理\n",
    "#     T.RandomRotation(15),  # 数据增强：随机旋转\n",
    "#     T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 位移\n",
    "#     T.ToTensor(), # 转换为Tensor\n",
    "#     # 如果需要标准化，可以取消注释以下行\n",
    "#     T.Normalize([0.5], [0.5])\n",
    "# ])\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self):\n",
    "        self.transform=A.Compose([\n",
    "            A.Resize(32, 32),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.Affine(translate_percent=(0.1,0.1),p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.Normalize(mean=(0.5,),std=(0.5,)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        img=np.array(img.convert('L'))\n",
    "        return self.transform(image=img)['image']\n",
    "\n",
    "transform=AlbumentationsTransform()\n",
    "\n",
    "# 定义划分数据集的函数\n",
    "def split_dataset(root_dir, transform, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, batch_size=128, shuffle=True, random_seed=42):\n",
    "    \"\"\"\n",
    "    划分数据集为训练集、验证集和测试集，并返回对应的DataLoader。\n",
    "\n",
    "    参数:\n",
    "    - root_dir: 数据集根目录\n",
    "    - transform: 数据预处理变换\n",
    "    - train_ratio: 训练集比例\n",
    "    - val_ratio: 验证集比例\n",
    "    - test_ratio: 测试集比例\n",
    "    - batch_size: 批次大小\n",
    "    - shuffle: 是否打乱数据\n",
    "    - random_seed: 随机种子，用于保证结果可重复\n",
    "\n",
    "    返回:\n",
    "    - train_loader: 训练集DataLoader\n",
    "    - val_loader: 验证集DataLoader\n",
    "    - test_loader: 测试集DataLoader\n",
    "    - full_dataset: 原始的ImageFolder数据集\n",
    "    \"\"\"\n",
    "    # 确保比例之和为1\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"比例之和必须为1\"\n",
    "\n",
    "    # 加载整个数据集\n",
    "    full_dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "    # 计算每个子集的大小\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    val_size = int(val_ratio * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    # 随机划分数据集\n",
    "    train_dataset, val_dataset, test_dataset = Data.random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "    # 创建DataLoader\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = Data.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, full_dataset\n",
    "\n"
   ],
   "id": "b359e60354209cf8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.563963Z",
     "start_time": "2025-04-20T03:09:35.396870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用函数划分数据集\n",
    "train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "    root_dir=\"EnglishHnd/English/Hnd/Img\",\n",
    "    transform=transform,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)\n",
    "\n",
    "# 数据可视化\n",
    "to_img = T.ToPILImage()\n",
    "a = to_img(train_loader.dataset[0][0])  # size=[1, 32, 32]\n",
    "plt.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(32*8*8, 400),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, label_num),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.Conv1(input)\n",
    "        input = self.Conv2(input)\n",
    "        input = input.view(input.size(0), -1)\n",
    "        output = self.Linear(input)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "# 定义损失函数\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "id": "8647acde17e4a36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 15406\n",
      "验证集大小: 3301\n",
      "测试集大小: 3303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (Conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Linear): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=400, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=400, out_features=80, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=80, out_features=62, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.587654Z",
     "start_time": "2025-04-20T03:09:35.576599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 轻量CNN结构？\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels // 2, 1)\n",
    "        self.conv2 = nn.Conv2d(channels // 2, channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.conv1(x)\n",
    "        attn = self.conv2(attn)\n",
    "        attn = self.sigmoid(attn)\n",
    "        return x * attn"
   ],
   "id": "ff9e5cbf56d5da0a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.630218Z",
     "start_time": "2025-04-20T03:09:35.624398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 添加残差模块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.attn = SimpleAttention(out_channels) if use_attention else nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out = self.attn(out)\n",
    "        return self.relu(out + res)"
   ],
   "id": "6516e022530947b0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.655721Z",
     "start_time": "2025-04-20T03:09:35.649361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加入倒残差模块\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_ratio=6, stride=1):\n",
    "        super().__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.use_res_connect = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.block(x)\n",
    "        else:\n",
    "            return self.block(x)"
   ],
   "id": "670c365e6c99fed8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.690604Z",
     "start_time": "2025-04-20T03:09:35.678654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN自注意力模块？\n",
    "class SelfAttention2D(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # 可学习的缩放系数\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        proj_q = self.query(x).view(B, -1, H * W)          # B x C1 x N\n",
    "        proj_k = self.key(x).view(B, -1, H * W)            # B x C1 x N\n",
    "        proj_v = self.value(x).view(B, -1, H * W)          # B x C  x N\n",
    "\n",
    "        attention = torch.bmm(proj_q.permute(0, 2, 1), proj_k)  # B x N x N\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        out = torch.bmm(proj_v, attention.permute(0, 2, 1))     # B x C x N\n",
    "        out = out.view(B, C, H, W)\n",
    "\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class CNNWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # → 16x16\n",
    "        )\n",
    "\n",
    "        # 增加额外卷积层 (2层)\n",
    "        # self.pre_extra = nn.Sequential(\n",
    "        #     nn.Conv2d(32, 32, 3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 32, 3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.block1 = ResidualBlock(32, 64, use_attention=True)\n",
    "        self.inverted_block = InvertedResidualBlock(64, 64, expansion_ratio=6)  # 倒残差模块\n",
    "        self.block2 = ResidualBlock(64, 128, use_attention=True)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 14x14 → 7x7\n",
    "        self.block3 = ResidualBlock(128, 128)\n",
    "        # self.block4 = ResidualBlock(128, 128)\n",
    "        # self.block5 = ResidualBlock(128, 128)\n",
    "        self.attention = SelfAttention2D(128) # 添加自注意力模块\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        # x = self.pre_extra(x)\n",
    "        x = self.extra_conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.inverted_block(x)  # 倒残差模块\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        # x = self.block4(x)\n",
    "        # x = self.block5(x)\n",
    "        x = self.attention(x)\n",
    "        return self.classifier(x)\n",
    "\n"
   ],
   "id": "e3e9cbd5d4d3fe67",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:35.715770Z",
     "start_time": "2025-04-20T03:09:35.704274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练与验证函数\n",
    "def train_and_validate(model, train_loader, val_loader, epochs, device='cpu', save_path='resnet18_best_model.pth', save_best_only=True):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "    best_val_top1_acc = 0.0  # 用于追踪验证集 Top-1 准确率的最佳值\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        top1_correct = 0\n",
    "        top3_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # --- 训练阶段 ---\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_func(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            # Top-1\n",
    "            _, top1_pred = torch.max(outputs, 1)\n",
    "            top1_correct += (top1_pred == batch_y).sum().item()\n",
    "\n",
    "            # Top-3\n",
    "            _, top3_pred_indices = torch.topk(outputs, 3, dim=1)\n",
    "            top3_correct += torch.sum(top3_pred_indices == batch_y.view(-1, 1).expand_as(top3_pred_indices)).item()\n",
    "\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        train_top1_acc = top1_correct / total\n",
    "        train_top3_acc = top3_correct / total\n",
    "\n",
    "        # --- 验证阶段 ---\n",
    "        model.eval()\n",
    "        val_top1_correct = 0\n",
    "        val_top3_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_outputs = model(val_x)\n",
    "                val_loss += loss_func(val_outputs, val_y).item() * val_x.size(0)\n",
    "\n",
    "                # Top-1\n",
    "                _, val_top1_pred = torch.max(val_outputs, 1)\n",
    "                val_top1_correct += (val_top1_pred == val_y).sum().item()\n",
    "\n",
    "                # Top-3\n",
    "                _, val_top3_pred_indices = torch.topk(val_outputs, 3, dim=1)\n",
    "                val_top3_correct += torch.sum(val_top3_pred_indices == val_y.view(-1, 1).expand_as(val_top3_pred_indices)).item()\n",
    "\n",
    "                val_total += val_y.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        val_top1_acc = val_top1_correct / val_total\n",
    "        val_top3_acc = val_top3_correct / val_total\n",
    "\n",
    "        # --- TensorBoard日志记录 ---\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train_top1\", train_top1_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train_top3\", train_top3_acc, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_top1\", val_top1_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_top3\", val_top3_acc, epoch)\n",
    "\n",
    "        # --- 控制台输出 ---\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}, \"\n",
    "              f\"Train Top-1 Acc: {train_top1_acc:.4f}, Train Top-3 Acc: {train_top3_acc:.4f}, \"\n",
    "              f\"Val Top-1 Acc: {val_top1_acc:.4f}, Val Top-3 Acc: {val_top3_acc:.4f}\")\n",
    "\n",
    "        # --- 保存最佳模型 ---\n",
    "        if save_best_only:\n",
    "            if val_top1_acc > best_val_top1_acc:\n",
    "                best_val_top1_acc = val_top1_acc\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"✅ 新的最佳模型已保存，Val Top-1 Acc: {val_top1_acc:.4f}\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    writer.close()"
   ],
   "id": "713c38679952a4c2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T03:09:49.467564Z",
     "start_time": "2025-04-20T03:09:35.727088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"当前使用设备: {device}\")\n",
    "\n",
    "    train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "        root_dir=ROOT_DIR,\n",
    "        transform=transform,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "    print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "    print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "    label_num = len(full_dataset.class_to_idx)\n",
    "    model = CNNWithAttention(label_num) # 调用模型\n",
    "\n",
    "    train_and_validate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCH,\n",
    "        device=device,\n",
    "        save_path=MODEL_SAVE_PATH\n",
    "    )\n",
    "\n",
    "    print(\"训练结束。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "e7e9e58fad621e9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备: cpu\n",
      "训练集大小: 32900\n",
      "验证集大小: 7050\n",
      "测试集大小: 7050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 35\u001B[39m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m训练结束。\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     20\u001B[39m label_num = \u001B[38;5;28mlen\u001B[39m(full_dataset.class_to_idx)\n\u001B[32m     21\u001B[39m model = CNNWithAttention(label_num) \u001B[38;5;66;03m# 调用模型\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mEPOCH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mMODEL_SAVE_PATH\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m训练结束。\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 18\u001B[39m, in \u001B[36mtrain_and_validate\u001B[39m\u001B[34m(model, train_loader, val_loader, epochs, device, save_path, save_best_only)\u001B[39m\n\u001B[32m     15\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# --- 训练阶段 ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    418\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001B[39m, in \u001B[36mDatasetFolder.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    237\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    238\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m    239\u001B[39m \u001B[33;03m    index (int): Index\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    242\u001B[39m \u001B[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[32m    243\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    244\u001B[39m path, target = \u001B[38;5;28mself\u001B[39m.samples[index]\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m sample = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    247\u001B[39m     sample = \u001B[38;5;28mself\u001B[39m.transform(sample)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001B[39m, in \u001B[36mdefault_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m284\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001B[39m, in \u001B[36mpil_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpil_loader\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) -> Image.Image:\n\u001B[32m    261\u001B[39m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mreturn\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mRGB\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fa3338f942a055f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
