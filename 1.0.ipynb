{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于卷积神经网络的手写英文字母识别系统研究",
   "id": "b49460515249296"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 准备数据集及数据预处理",
   "id": "ce590e5f75620cf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 将下载的数据集按类重命名",
   "id": "238bfe846709a109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from torchvision.datasets import EMNIST  \n",
    "from PIL import Image  \n",
    "import os  \n",
    "from IPython.display import display  \n",
    "\n",
    "def save_emnist_images_with_class_previews(save_dir=\"emnist_png_balanced\", max_images_per_class=1000, preview=True, preview_limit_per_class=10):  \n",
    "    digits = [str(i) for i in range(10)]  \n",
    "    letters = [  \n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',  \n",
    "        'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',  \n",
    "        'a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't'  \n",
    "    ]  \n",
    "    all_labels = digits + letters  \n",
    "\n",
    "    # 修改后的函数，区分大小写文件夹名  \n",
    "    def label_char_to_folder_name(label_char):  \n",
    "        if label_char.islower():  \n",
    "            return label_char + \"_\"   # 小写加下划线后缀  \n",
    "        else:  \n",
    "            return label_char         # 大写和数字保持不变  \n",
    "\n",
    "    def label_to_char(label):  \n",
    "        if 0 <= label < len(all_labels):  \n",
    "            return all_labels[label]  \n",
    "        else:  \n",
    "            return \"UNK\"  \n",
    "\n",
    "    # 先处理预览部分：读取时用转换后的文件夹名  \n",
    "    if os.path.exists(save_dir) and os.path.isdir(save_dir):  \n",
    "        print(f\"检测到目录 {save_dir} 已存在，跳过下载，直接展示预览图...\")  \n",
    "        class_preview_images = {}  \n",
    "        for label_char in all_labels:  \n",
    "            folder_name = label_char_to_folder_name(label_char)  \n",
    "            label_dir = os.path.join(save_dir, folder_name)  \n",
    "            imgs = []  \n",
    "            if os.path.exists(label_dir) and os.path.isdir(label_dir):  \n",
    "                files = sorted(os.listdir(label_dir))  \n",
    "                for f in files[:preview_limit_per_class]:  \n",
    "                    try:  \n",
    "                        im = Image.open(os.path.join(label_dir, f)).convert('L')  \n",
    "                        imgs.append(im)  \n",
    "                    except:  \n",
    "                        pass  \n",
    "            if imgs:  \n",
    "                class_preview_images[label_char] = imgs  \n",
    "        if preview:  \n",
    "            for label_char, imgs in class_preview_images.items():  \n",
    "                width = 28 * len(imgs)  \n",
    "                height = 28  \n",
    "                big_img = Image.new('L', (width, height))  \n",
    "                for idx, im in enumerate(imgs):  \n",
    "                    big_img.paste(im, (28*idx, 0))  \n",
    "                print(f\"类别 {label_char} 预览图 (共 {len(imgs)} 张):\")  \n",
    "                display(big_img)  \n",
    "\n",
    "        print(\"预览完毕。\")  \n",
    "        return  \n",
    "\n",
    "    # 下载并保存图像  \n",
    "    print(f\"目录 {save_dir} 不存在，开始下载数据并保存...\")  \n",
    "    train_set = EMNIST(root=\"emnist_data\", split=\"balanced\", train=True, download=True)  \n",
    "    test_set = EMNIST(root=\"emnist_data\", split=\"balanced\", train=False, download=True)  \n",
    "    full_dataset = train_set + test_set  \n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "    images_per_class = {label_char: 0 for label_char in all_labels}  \n",
    "    saved_images_count = 0  \n",
    "\n",
    "    class_preview_images = {label_char: [] for label_char in all_labels} if preview else None  \n",
    "\n",
    "    for i, (img, label) in enumerate(full_dataset):  \n",
    "        label_char = label_to_char(label)  \n",
    "        if label_char == \"UNK\":  \n",
    "            continue  \n",
    "        if images_per_class[label_char] >= max_images_per_class:  \n",
    "            continue  \n",
    "\n",
    "        folder_name = label_char_to_folder_name(label_char)  \n",
    "        label_dir = os.path.join(save_dir, folder_name)  \n",
    "        os.makedirs(label_dir, exist_ok=True)  \n",
    "\n",
    "        img = img.resize((28, 28))  \n",
    "        img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT).rotate(90)  \n",
    "        img.save(os.path.join(label_dir, f\"{images_per_class[label_char]}.png\"))  \n",
    "\n",
    "        if preview and len(class_preview_images[label_char]) < preview_limit_per_class:  \n",
    "            class_preview_images[label_char].append(img)  \n",
    "\n",
    "        images_per_class[label_char] += 1  \n",
    "        saved_images_count += 1  \n",
    "\n",
    "        if saved_images_count % 500 == 0:  \n",
    "            print(f\"已保存 {saved_images_count} 张图像...\")  \n",
    "\n",
    "    print(\"所有图像已保存到\", save_dir)  \n",
    "\n",
    "    if preview:  \n",
    "        for label_char, imgs in class_preview_images.items():  \n",
    "            if len(imgs) == 0:  \n",
    "                continue  \n",
    "            width = 28 * len(imgs)  \n",
    "            height = 28  \n",
    "            big_img = Image.new('L', (width, height))  \n",
    "            for idx, im in enumerate(imgs):  \n",
    "                big_img.paste(im, (28*idx, 0))  \n",
    "            print(f\"类别 {label_char} 预览图 (共 {len(imgs)} 张):\")  \n",
    "            display(big_img)  \n",
    "\n",
    "# 调用（Notebook中执行）  \n",
    "save_emnist_images_with_class_previews(save_dir=\"emnist_png_balanced\", max_images_per_class=1000, preview=True, preview_limit_per_class=10)  \n",
    "\n"
   ],
   "id": "5788fac3ecac17f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 转白底黑字",
   "id": "1780fbeb3c8485f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import os  \n",
    "# from PIL import Image  \n",
    "# import numpy as np  \n",
    "# \n",
    "# def invert_image_colors_inplace(image_path):  \n",
    "#     \"\"\"原地反转图片颜色：黑底白字 <-> 白底黑字\"\"\"  \n",
    "#     image = Image.open(image_path).convert(\"RGB\")  \n",
    "#     img_array = np.array(image)  \n",
    "#     inverted_array = 255 - img_array  \n",
    "#     inverted_image = Image.fromarray(inverted_array)  \n",
    "#     inverted_image.save(image_path)  \n",
    "#     print(f\"Inverted and overwritten: {image_path}\")  \n",
    "# \n",
    "# def process_folder_recursive_inplace(root_folder):  \n",
    "#     for root, dirs, files in os.walk(root_folder):  \n",
    "#         for file in files:  \n",
    "#             if not file.lower().endswith(('.png', '.jpg', '.jpeg')):  \n",
    "#                 continue  \n",
    "# \n",
    "#             file_name_without_ext = os.path.splitext(file)[0]  \n",
    "# \n",
    "#             # 判断图片名是否为纯数字，是则反转颜色  \n",
    "#             if file_name_without_ext.isdigit():  \n",
    "#                 img_path = os.path.join(root, file)  \n",
    "#                 invert_image_colors_inplace(img_path)  \n",
    "# \n",
    "#             # 如果图片名以 \"img\" 开头 (忽略大小写)，则跳过处理，保持原样  \n",
    "#             elif file_name_without_ext.lower().startswith(\"img\"):  \n",
    "#                 # 不做处理，跳过  \n",
    "#                 print(f\"Skipped (img prefix): {os.path.join(root, file)}\")  \n",
    "#                 continue  \n",
    "# \n",
    "#             else:  \n",
    "#                 # 其他图片，根据需求这里先跳过，或者也可以反转  \n",
    "#                 print(f\"Skipped (other): {os.path.join(root, file)}\")  \n",
    "#                 continue  \n",
    "# \n",
    "# if __name__ == \"__main__\":  \n",
    "#     img_root_folder = r\"emnist_png_balanced\"  \n",
    "#     process_folder_recursive_inplace(img_root_folder)  "
   ],
   "id": "2660e0fd2525a649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 划分训练集和测试集",
   "id": "59a98459a3c1efda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:17:22.162375Z",
     "start_time": "2025-04-21T06:17:11.020814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torchvision import utils\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as Data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "#使用tensorboardX进行可视化\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "#数据增强\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms.autoaugment import AutoAugmentPolicy #自动数据增强库\n",
    "import albumentations as A\n",
    "#参数定义\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "ROOT_DIR = \"emnist_png_balanced\"\n",
    "LOG_DIR = f\"runs/handwriting_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "MODEL_SAVE_PATH = \"cnn_res_attention_best.pth\""
   ],
   "id": "6e8d3e7ce616fde7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 数据增强模块\n",
    "# transform = T.Compose([\n",
    "#     T.Resize((28, 28)), # 统一尺寸为28x28\n",
    "#     T.Grayscale(num_output_channels=1), # 灰度处理\n",
    "#     T.RandomRotation(15),  # 数据增强：随机旋转\n",
    "#     T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 位移\n",
    "#     T.ToTensor(), # 转换为Tensor\n",
    "#     # 如果需要标准化，可以取消注释以下行\n",
    "#     T.Normalize([0.5], [0.5])\n",
    "# ])\n",
    "\n",
    "# 存在图片过度偏移问题。\n",
    "# class DataAutoAugment:\n",
    "#     def __init__(self):\n",
    "#         self.transform = T.Compose([\n",
    "#             T.Resize((28, 28)),\n",
    "#             T.Grayscale(num_output_channels=1),\n",
    "#             T.AutoAugment(policy=AutoAugmentPolicy.SVHN),\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(mean=(0.5,), std=(0.5,))\n",
    "#         ])\n",
    "#\n",
    "#     def __call__(self, img):\n",
    "#         return self.transform(img)\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self):\n",
    "        self.transform=A.Compose([\n",
    "            A.Resize(28, 28),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.Affine(translate_percent=(0.1,0.1),p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.Normalize(mean=(0.5,),std=(0.5,)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    def __call__(self, img):\n",
    "        img=np.array(img.convert('L'))\n",
    "        return self.transform(image=img)['image']\n",
    "\n",
    "\n",
    "transform=AlbumentationsTransform()"
   ],
   "id": "f67c0a39abd6d4b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def split_dataset(root_dir, transform, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, batch_size=128, shuffle=True, random_seed=42): \n",
    "    \n",
    "    \"\"\"\n",
    "    划分数据集为训练集、验证集和测试集，并返回对应的DataLoader。\n",
    "\n",
    "    参数:\n",
    "    - root_dir: 数据集根目录\n",
    "    - transform: 数据预处理变换\n",
    "    - train_ratio: 训练集比例\n",
    "    - val_ratio: 验证集比例\n",
    "    - test_ratio: 测试集比例\n",
    "    - batch_size: 批次大小\n",
    "    - shuffle: 是否打乱数据\n",
    "    - random_seed: 随机种子，用于保证结果可重复\n",
    "\n",
    "    返回:\n",
    "    - train_loader: 训练集DataLoader\n",
    "    - val_loader: 验证集DataLoader\n",
    "    - test_loader: 测试集DataLoader\n",
    "    - full_dataset: 原始的ImageFolder数据集\n",
    "    \"\"\"\n",
    "    # 确保比例之和为1\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"比例之和必须为1\"  \n",
    "\n",
    "    full_dataset = datasets.ImageFolder(root=root_dir, transform=transform)  \n",
    "    targets = full_dataset.targets  \n",
    "    \n",
    "    class_indices = {}  \n",
    "    for idx, label in enumerate(targets):  \n",
    "        class_indices.setdefault(label, []).append(idx)  \n",
    "    \n",
    "    train_indices = []  \n",
    "    val_indices = []  \n",
    "    test_indices = []  \n",
    "    \n",
    "    generator = torch.Generator().manual_seed(random_seed)  \n",
    "    \n",
    "    for label, indices in class_indices.items():  \n",
    "        indices = torch.tensor(indices)  \n",
    "        indices = indices[torch.randperm(len(indices), generator=generator)]  \n",
    "        \n",
    "        n_total = len(indices)  \n",
    "        n_train = int(train_ratio * n_total)  \n",
    "        n_val = int(val_ratio * n_total)  \n",
    "        n_test = n_total - n_train - n_val  \n",
    "        \n",
    "        train_indices.extend(indices[:n_train].tolist())  \n",
    "        val_indices.extend(indices[n_train:n_train+n_val].tolist())  \n",
    "        test_indices.extend(indices[n_train+n_val:].tolist())  \n",
    "    \n",
    "    train_subset = Subset(full_dataset, train_indices)  \n",
    "    val_subset = Subset(full_dataset, val_indices)  \n",
    "    test_subset = Subset(full_dataset, test_indices)  \n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=shuffle)  \n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)  \n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "    # 统计每个类别数量的函数  \n",
    "    def count_classes(indices, targets):  \n",
    "        label_counts = Counter()  \n",
    "        for idx in indices:  \n",
    "            label_counts[targets[idx]] += 1  \n",
    "        return label_counts  \n",
    "\n",
    "    # 打印每个类别样本数  \n",
    "    train_counts = count_classes(train_indices, targets)  \n",
    "    val_counts = count_classes(val_indices, targets)  \n",
    "    test_counts = count_classes(test_indices, targets)  \n",
    "    \n",
    "    classes = full_dataset.classes  # 类别名称列表  \n",
    "    \n",
    "    print(\"训练集每个类别样本数：\")  \n",
    "    for label_idx, count in sorted(train_counts.items()):  \n",
    "        print(f\"类别 {classes[label_idx]}: {count}\")  \n",
    "        \n",
    "    print(\"\\n验证集每个类别样本数：\")  \n",
    "    for label_idx, count in sorted(val_counts.items()):  \n",
    "        print(f\"类别 {classes[label_idx]}: {count}\")  \n",
    "        \n",
    "    print(\"\\n测试集每个类别样本数：\")  \n",
    "    for label_idx, count in sorted(test_counts.items()):  \n",
    "        print(f\"类别 {classes[label_idx]}: {count}\")  \n",
    "\n",
    "    return train_loader, val_loader, test_loader, full_dataset \n"
   ],
   "id": "fe1425714846c0e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用函数划分数据集\n",
    "train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    transform=transform,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)\n",
    "\n",
    "# 数据可视化\n",
    "to_img = T.ToPILImage()\n",
    "a = to_img(train_loader.dataset[0][0])  # size=[1, 32, 32]\n",
    "plt.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(32*7*7, 400),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80, label_num),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.Conv1(input)\n",
    "        input = self.Conv2(input)\n",
    "        input = input.view(input.size(0), -1)\n",
    "        output = self.Linear(input)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "# 定义损失函数\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "id": "f9442b6646ea1ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 轻量CNN结构？\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        # Squeeze操作：全局平均池化\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Excitation操作：两个全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()  # 输入形状 [B, C, H, W]\n",
    "        # Squeeze\n",
    "        y = self.avg_pool(x).view(b, c)  # [B, C]\n",
    "        # Excitation\n",
    "        y = self.fc(y).view(b, c, 1, 1)  # [B, C, 1, 1]\n",
    "        # Reweight\n",
    "        return x * y.expand_as(x)  # 广播乘法 [B, C, H, W]\n"
   ],
   "id": "808f797243c4d9c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 自注意力模块\n",
    "# class SelfAttention2D(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super().__init__()\n",
    "#         self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "#         self.key   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "#         self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "#         self.gamma = nn.Parameter(torch.zeros(1))  # 可学习的缩放系数\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.shape\n",
    "#         proj_q = self.query(x).view(B, -1, H * W)          # B x C1 x N\n",
    "#         proj_k = self.key(x).view(B, -1, H * W)            # B x C1 x N\n",
    "#         proj_v = self.value(x).view(B, -1, H * W)          # B x C  x N\n",
    "#\n",
    "#         attention = torch.bmm(proj_q.permute(0, 2, 1), proj_k)  # B x N x N\n",
    "#         attention = torch.softmax(attention, dim=-1)\n",
    "#\n",
    "#         out = torch.bmm(proj_v, attention.permute(0, 2, 1))     # B x C x N\n",
    "#         out = out.view(B, C, H, W)\n",
    "#\n",
    "#         return self.gamma * out + x\n",
    "\n",
    "# class CBAM(nn.Module):\n",
    "#     def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "#         super().__init__()\n",
    "#         # 通道注意力\n",
    "#         self.channel_att = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(1),\n",
    "#             nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(channels // reduction, channels, 1, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#         # 空间注意力\n",
    "#         self.spatial_att = nn.Sequential(\n",
    "#             nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # 通道注意力\n",
    "#         ca = self.channel_att(x)\n",
    "#         x = x * ca\n",
    "#\n",
    "#         # 空间注意力\n",
    "#         max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "#         avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "#         sa_input = torch.cat([avg_pool, max_pool], dim=1)\n",
    "#         sa = self.spatial_att(sa_input)\n",
    "#         x = x * sa\n",
    "#\n",
    "#         return x\n"
   ],
   "id": "e5fa26a8ceadf759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 添加残差模块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_attention=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        #调用注意力机制\n",
    "        self.attn = SimpleAttention(out_channels) if use_attention else nn.Identity()\n",
    "        # self.attn = CBAM(out_channels) if use_attention else nn.Identity()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.shortcut(x)\n",
    "        out = self.conv(x)\n",
    "        out = self.attn(out)\n",
    "        return self.relu(out + res)"
   ],
   "id": "8882f3328c105096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加入倒残差模块\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_ratio=6, stride=1, use_attention=False):\n",
    "        super().__init__()\n",
    "        hidden_dim = in_channels * expansion_ratio\n",
    "        self.use_res_connect = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        # 调用注意力模块\n",
    "        # self.attn = CBAM(out_channels) if use_attention else nn.Identity()\n",
    "        self.attn = SimpleAttention(out_channels) if use_attention else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        out = self.attn(out)\n",
    "        if self.use_res_connect:\n",
    "            return x + out\n",
    "        else:\n",
    "            return out\n"
   ],
   "id": "f50e469f27c984fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CNN与注意力模块的结合\n",
    "class CNNWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # → 16x16\n",
    "        )\n",
    "\n",
    "        # 增加额外卷积层 (2层)\n",
    "        # self.pre_extra = nn.Sequential(\n",
    "        #     nn.Conv2d(32, 32, 3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 32, 3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.block1 = ResidualBlock(32, 64, use_attention=True)\n",
    "        self.inverted_block = InvertedResidualBlock(64, 64, expansion_ratio=6, use_attention=True)  # 倒残差模块\n",
    "        self.block2 = ResidualBlock(64, 128, use_attention=True)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # 14x14 → 7x7\n",
    "        self.block3 = ResidualBlock(128, 128)\n",
    "        # self.block4 = ResidualBlock(128, 128)\n",
    "        # self.block5 = ResidualBlock(128, 128)\n",
    "        # self.attention = SelfAttention2D(128) # 添加自注意力模块\n",
    "        self.attention=SimpleAttention(128)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        # x = self.pre_extra(x)\n",
    "        x = self.extra_conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.inverted_block(x)  # 倒残差模块\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        # x = self.block4(x)\n",
    "        # x = self.block5(x)\n",
    "        x = self.attention(x)\n",
    "        return self.classifier(x)\n"
   ],
   "id": "2720e841c3bfba97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 训练与验证函数\n",
    "def train_and_validate(model, train_loader, val_loader, epochs, device='cpu', save_path='resnet18_best_model.pth', save_best_only=True):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "    best_val_top1_acc = 0.0  # 用于追踪验证集 Top-1 准确率的最佳值\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        top1_correct = 0\n",
    "        top3_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # --- 训练阶段 ---\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_func(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            # Top-1\n",
    "            _, top1_pred = torch.max(outputs, 1)\n",
    "            top1_correct += (top1_pred == batch_y).sum().item()\n",
    "\n",
    "            # Top-3\n",
    "            _, top3_pred_indices = torch.topk(outputs, 3, dim=1)\n",
    "            top3_correct += torch.sum(top3_pred_indices == batch_y.view(-1, 1).expand_as(top3_pred_indices)).item()\n",
    "\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        train_top1_acc = top1_correct / total\n",
    "        train_top3_acc = top3_correct / total\n",
    "\n",
    "        # --- 验证阶段 ---\n",
    "        model.eval()\n",
    "        val_top1_correct = 0\n",
    "        val_top3_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_outputs = model(val_x)\n",
    "                val_loss += loss_func(val_outputs, val_y).item() * val_x.size(0)\n",
    "\n",
    "                # Top-1\n",
    "                _, val_top1_pred = torch.max(val_outputs, 1)\n",
    "                val_top1_correct += (val_top1_pred == val_y).sum().item()\n",
    "\n",
    "                # Top-3\n",
    "                _, val_top3_pred_indices = torch.topk(val_outputs, 3, dim=1)\n",
    "                val_top3_correct += torch.sum(val_top3_pred_indices == val_y.view(-1, 1).expand_as(val_top3_pred_indices)).item()\n",
    "\n",
    "                val_total += val_y.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        val_top1_acc = val_top1_correct / val_total\n",
    "        val_top3_acc = val_top3_correct / val_total\n",
    "\n",
    "        # --- TensorBoard日志记录 ---\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train_top1\", train_top1_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train_top3\", train_top3_acc, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_top1\", val_top1_acc, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val_top3\", val_top3_acc, epoch)\n",
    "\n",
    "        # --- 控制台输出 ---\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}, \"\n",
    "              f\"Train Top-1 Acc: {train_top1_acc:.4f}, Train Top-3 Acc: {train_top3_acc:.4f}, \"\n",
    "              f\"Val Top-1 Acc: {val_top1_acc:.4f}, Val Top-3 Acc: {val_top3_acc:.4f}\")\n",
    "\n",
    "        # --- 保存最佳模型 ---\n",
    "        if save_best_only:\n",
    "            if val_top1_acc > best_val_top1_acc:\n",
    "                best_val_top1_acc = val_top1_acc\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"✅ 新的最佳模型已保存，Val Top-1 Acc: {val_top1_acc:.4f}\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    writer.close()"
   ],
   "id": "d194f2ee805c6626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"当前使用设备: {device}\")\n",
    "\n",
    "    train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "        root_dir=ROOT_DIR,\n",
    "        transform=transform,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        test_ratio=0.15,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "    print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "    print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "    label_num = len(full_dataset.class_to_idx)\n",
    "    model = CNNWithAttention(label_num) # 调用模型\n",
    "\n",
    "    train_and_validate(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCH,\n",
    "        device=device,\n",
    "        save_path=MODEL_SAVE_PATH\n",
    "    )\n",
    "\n",
    "    print(\"训练结束。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "7c73bc789582f738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": " ",
   "id": "807665a7393e5942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "28d7cf40b959bee7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
