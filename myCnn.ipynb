{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于卷积神经网络的手写英文字母识别系统研究",
   "id": "553978cdf843cfcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:07:57.342616Z",
     "start_time": "2025-04-22T13:07:57.327659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from myCnn.cnnWithAttention.utils import *\n",
    "from myCnn.cnnWithAttention.attentionStructure import *\n",
    "from myCnn.cnnWithAttention.train_my_cnn import *"
   ],
   "id": "acd8cdf4eee54e5f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基本参数设置",
   "id": "2588026462704b14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:08:01.011207Z",
     "start_time": "2025-04-22T13:08:01.003836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROOT_DIR = \"emnist_png_balanced\"  # 数据集路径\n",
    "BATCH_SIZE = 256  # 批大小\n",
    "EPOCH = 500  # 训练轮数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# 设备选择\n",
    "LR = 1e-4  # 学习率"
   ],
   "id": "52a37f0c142830aa",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 加入数据增强",
   "id": "ba0f318b695458e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:08:03.680743Z",
     "start_time": "2025-04-22T13:08:03.456060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = AlbumentationsTransform()  # 使用数据增强\n",
    "\n",
    "train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    transform=transform,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)\n",
    "\n",
    "# 数据可视化\n",
    "to_img = T.ToPILImage()\n",
    "a = to_img(train_loader.dataset[0][0])  # size=[1, 32, 32]\n",
    "plt.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "a81c0144afddb0a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集每个类别样本数：\n",
      "类别 0: 700\n",
      "类别 1: 700\n",
      "类别 2: 700\n",
      "类别 3: 700\n",
      "类别 4: 700\n",
      "类别 5: 700\n",
      "类别 6: 700\n",
      "类别 7: 700\n",
      "类别 8: 700\n",
      "类别 9: 700\n",
      "类别 A: 700\n",
      "类别 B: 700\n",
      "类别 C: 700\n",
      "类别 D: 700\n",
      "类别 E: 700\n",
      "类别 F: 700\n",
      "类别 G: 700\n",
      "类别 H: 700\n",
      "类别 I: 700\n",
      "类别 J: 700\n",
      "类别 K: 700\n",
      "类别 L: 700\n",
      "类别 M: 700\n",
      "类别 N: 700\n",
      "类别 O: 700\n",
      "类别 P: 700\n",
      "类别 Q: 700\n",
      "类别 R: 700\n",
      "类别 S: 700\n",
      "类别 T: 700\n",
      "类别 U: 700\n",
      "类别 V: 700\n",
      "类别 W: 700\n",
      "类别 X: 700\n",
      "类别 Y: 700\n",
      "类别 Z: 700\n",
      "类别 a_: 700\n",
      "类别 b_: 700\n",
      "类别 d_: 700\n",
      "类别 e_: 700\n",
      "类别 f_: 700\n",
      "类别 g_: 700\n",
      "类别 h_: 700\n",
      "类别 n_: 700\n",
      "类别 q_: 700\n",
      "类别 r_: 700\n",
      "类别 t_: 700\n",
      "\n",
      "验证集每个类别样本数：\n",
      "类别 0: 150\n",
      "类别 1: 150\n",
      "类别 2: 150\n",
      "类别 3: 150\n",
      "类别 4: 150\n",
      "类别 5: 150\n",
      "类别 6: 150\n",
      "类别 7: 150\n",
      "类别 8: 150\n",
      "类别 9: 150\n",
      "类别 A: 150\n",
      "类别 B: 150\n",
      "类别 C: 150\n",
      "类别 D: 150\n",
      "类别 E: 150\n",
      "类别 F: 150\n",
      "类别 G: 150\n",
      "类别 H: 150\n",
      "类别 I: 150\n",
      "类别 J: 150\n",
      "类别 K: 150\n",
      "类别 L: 150\n",
      "类别 M: 150\n",
      "类别 N: 150\n",
      "类别 O: 150\n",
      "类别 P: 150\n",
      "类别 Q: 150\n",
      "类别 R: 150\n",
      "类别 S: 150\n",
      "类别 T: 150\n",
      "类别 U: 150\n",
      "类别 V: 150\n",
      "类别 W: 150\n",
      "类别 X: 150\n",
      "类别 Y: 150\n",
      "类别 Z: 150\n",
      "类别 a_: 150\n",
      "类别 b_: 150\n",
      "类别 d_: 150\n",
      "类别 e_: 150\n",
      "类别 f_: 150\n",
      "类别 g_: 150\n",
      "类别 h_: 150\n",
      "类别 n_: 150\n",
      "类别 q_: 150\n",
      "类别 r_: 150\n",
      "类别 t_: 150\n",
      "\n",
      "测试集每个类别样本数：\n",
      "类别 0: 150\n",
      "类别 1: 150\n",
      "类别 2: 150\n",
      "类别 3: 150\n",
      "类别 4: 150\n",
      "类别 5: 150\n",
      "类别 6: 150\n",
      "类别 7: 150\n",
      "类别 8: 150\n",
      "类别 9: 150\n",
      "类别 A: 150\n",
      "类别 B: 150\n",
      "类别 C: 150\n",
      "类别 D: 150\n",
      "类别 E: 150\n",
      "类别 F: 150\n",
      "类别 G: 150\n",
      "类别 H: 150\n",
      "类别 I: 150\n",
      "类别 J: 150\n",
      "类别 K: 150\n",
      "类别 L: 150\n",
      "类别 M: 150\n",
      "类别 N: 150\n",
      "类别 O: 150\n",
      "类别 P: 150\n",
      "类别 Q: 150\n",
      "类别 R: 150\n",
      "类别 S: 150\n",
      "类别 T: 150\n",
      "类别 U: 150\n",
      "类别 V: 150\n",
      "类别 W: 150\n",
      "类别 X: 150\n",
      "类别 Y: 150\n",
      "类别 Z: 150\n",
      "类别 a_: 150\n",
      "类别 b_: 150\n",
      "类别 d_: 150\n",
      "类别 e_: 150\n",
      "类别 f_: 150\n",
      "类别 g_: 150\n",
      "类别 h_: 150\n",
      "类别 n_: 150\n",
      "类别 q_: 150\n",
      "类别 r_: 150\n",
      "类别 t_: 150\n",
      "训练集大小: 32900\n",
      "验证集大小: 7050\n",
      "测试集大小: 7050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 训练模型（有注意力机制）",
   "id": "ccb787c7a53d124a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T13:09:15.179938Z",
     "start_time": "2025-04-22T13:08:42.495438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNNWithAttention(label_num, use_attention=True) # 调用模型\n",
    "print(model)\n",
    "\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCH,\n",
    "    device=device,\n",
    "    save_path=\"cnn_res_attention_aug_best.pth\",\n",
    ")\n",
    "\n",
    "print(\"训练结束。\")"
   ],
   "id": "b0986ea8c8f86262",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNWithAttention(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (residual): ResidualBlock(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (attn): SimpleAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (shortcut): Sequential()\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (inverted_residual): InvertedResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU6(inplace=True)\n",
      "      (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (attn): SimpleAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attention): SelfAttention2D(\n",
      "    (query): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (key): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (value): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (fcon1): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=148, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (fcon2): Linear(in_features=148, out_features=47, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m model = CNNWithAttention(label_num, use_attention=\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# 调用模型\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(model)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mEPOCH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcnn_res_attention_aug_best.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m训练结束。\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\myCnn\\cnnWithAttention\\train_my_cnn.py:32\u001B[39m, in \u001B[36mtrain_and_validate\u001B[39m\u001B[34m(model, train_loader, val_loader, epochs, device, save_path, save_best_only, patience, auto_lr, lr, LOG_DIR)\u001B[39m\n\u001B[32m     29\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# --- 训练阶段 ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    418\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001B[39m, in \u001B[36mDatasetFolder.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    237\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    238\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m    239\u001B[39m \u001B[33;03m    index (int): Index\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    242\u001B[39m \u001B[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[32m    243\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    244\u001B[39m path, target = \u001B[38;5;28mself\u001B[39m.samples[index]\n\u001B[32m--> \u001B[39m\u001B[32m245\u001B[39m sample = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    247\u001B[39m     sample = \u001B[38;5;28mself\u001B[39m.transform(sample)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001B[39m, in \u001B[36mdefault_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m284\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001B[39m, in \u001B[36mpil_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpil_loader\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) -> Image.Image:\n\u001B[32m    261\u001B[39m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m    263\u001B[39m         img = Image.open(f)\n\u001B[32m    264\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m img.convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 训练模型（无注意力机制）",
   "id": "ffa9536746cc0b39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CNNWithAttention(label_num, use_attention=False) # 调用模型\n",
    "\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCH,\n",
    "    device=device,\n",
    "    save_path=\"cnn_res_noattention_aug_best.pth\"\n",
    ")"
   ],
   "id": "abb212bc14be649c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 无数据增强",
   "id": "a08cd8dc8310aac9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = AlbumentationsTransformBase()  # 不使用数据增强\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    transform=transform,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)\n",
    "\n",
    "# 数据可视化\n",
    "to_img = T.ToPILImage()\n",
    "a = to_img(train_loader.dataset[0][0])  # size=[1, 32, 32]\n",
    "plt.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "884b710e5601641c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 训练模型（有注意力机制）",
   "id": "570e9cd41429298f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CNNWithAttention(label_num, use_attention=True) # 调用模型\n",
    "\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCH,\n",
    "    device=device,\n",
    "    save_path=\"cnn_res_attention_noaug_best.pth\",\n",
    "    lr=LR\n",
    ")"
   ],
   "id": "51e75e8955bf5ee6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 训练模型（无注意力机制）",
   "id": "6f3ddb4d7a41a227"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CNNWithAttention(label_num, use_attention=False) # 调用模型\n",
    "\n",
    "train_and_validate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCH,\n",
    "    device=device,\n",
    "    save_path=\"cnn_res_noattention_noaug_best.pth\",\n",
    "    lr=LR\n",
    ")"
   ],
   "id": "90651f1fb456153a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "compy12",
   "language": "python",
   "display_name": "Python (Compy12)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
