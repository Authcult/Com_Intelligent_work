{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于卷积神经网络的手写英文字母识别系统研究",
   "id": "5646077bb4c6c51a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 准备数据集及数据预处理",
   "id": "aff1bb67f4350589"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 将下载的数据集按类重命名",
   "id": "19ecd33f573bfd16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T00:23:57.123713Z",
     "start_time": "2025-04-16T00:23:57.116374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\"\"\n",
    "import os\n",
    "import string\n",
    "\n",
    "# 定义源目录路径\n",
    "source_dir = \"EnglishImg/EnglishImg/English/Img/GoodImg/Bmp\"\n",
    "\n",
    "# 生成目标文件夹名称列表\n",
    "target_folders = list(string.digits) + list(string.ascii_uppercase) + [f\"{char}_\" for char in string.ascii_lowercase]\n",
    "\n",
    "# 获取源目录下的所有文件夹名称\n",
    "source_folders = sorted([f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))])\n",
    "\n",
    "# 确保源文件夹数量与目标文件夹数量一致\n",
    "if len(source_folders) != len(target_folders):\n",
    "    raise ValueError(\"源文件夹数量与目标文件夹数量不一致\")\n",
    "\n",
    "# 重命名文件夹\n",
    "for source_folder, target_folder in zip(source_folders, target_folders):\n",
    "    source_path = os.path.join(source_dir, source_folder)\n",
    "    target_path = os.path.join(source_dir, target_folder)\n",
    "\n",
    "    try:\n",
    "        os.rename(source_path, target_path)\n",
    "        print(f\"重命名: {source_path} -> {target_path}\")\n",
    "    except FileExistsError:\n",
    "        print(f\"目标文件夹 {target_path} 已存在，跳过重命名 {source_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"重命名 {source_path} 到 {target_path} 时出错: {e}\")\n",
    "\"\"\"\"\"\n",
    "\n",
    "\n"
   ],
   "id": "c11beca2957d29a1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nimport os\\nimport string\\n\\n# 定义源目录路径\\nsource_dir = \"EnglishImg/EnglishImg/English/Img/GoodImg/Bmp\"\\n\\n# 生成目标文件夹名称列表\\ntarget_folders = list(string.digits) + list(string.ascii_uppercase) + [f\"{char}_\" for char in string.ascii_lowercase]\\n\\n# 获取源目录下的所有文件夹名称\\nsource_folders = sorted([f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))])\\n\\n# 确保源文件夹数量与目标文件夹数量一致\\nif len(source_folders) != len(target_folders):\\n    raise ValueError(\"源文件夹数量与目标文件夹数量不一致\")\\n\\n# 重命名文件夹\\nfor source_folder, target_folder in zip(source_folders, target_folders):\\n    source_path = os.path.join(source_dir, source_folder)\\n    target_path = os.path.join(source_dir, target_folder)\\n\\n    try:\\n        os.rename(source_path, target_path)\\n        print(f\"重命名: {source_path} -> {target_path}\")\\n    except FileExistsError:\\n        print(f\"目标文件夹 {target_path} 已存在，跳过重命名 {source_path}\")\\n    except Exception as e:\\n        print(f\"重命名 {source_path} 到 {target_path} 时出错: {e}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 划分训练集和测试集",
   "id": "204be173b2fc5723"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T00:26:07.344581Z",
     "start_time": "2025-04-16T00:26:07.294012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from myCnn.baseline.LeNet5 import LeNet5\n",
    "from myCnn.train_model import train_model\n",
    "from myCnn.utils import split_dataset\n",
    "from myCnn.baseline.resnet18 import resnet18\n",
    "from myCnn.baseline.mobilenet_v2 import mobilenet_v2\n",
    "from myCnn.baseline.svm_model import train_svm, flatten_images\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4"
   ],
   "id": "2cfdc1122596af3f",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmyCnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbaseline\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mresnet18\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m resnet18\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmyCnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbaseline\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmobilenet_v2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m mobilenet_v2\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmyCnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbaseline\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msvm_model\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_svm, flatten_images\n\u001B[32m     11\u001B[39m EPOCH = \u001B[32m100\u001B[39m\n\u001B[32m     12\u001B[39m BATCH_SIZE = \u001B[32m128\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Com_Intelligent_work\\myCnn\\baseline\\svm_model.py:2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m svm\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'sklearn'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义数据预处理变换\n",
    "transform = T.Compose([\n",
    "    T.Resize((32, 32)),\n",
    "    # T.Grayscale(num_output_channels=1),\n",
    "    T.ToTensor(),\n",
    "    # 如果需要标准化，可以取消注释以下行\n",
    "    # T.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n"
   ],
   "id": "c52de9fee7d42a30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用函数划分数据集\n",
    "train_loader, val_loader, test_loader, full_dataset = split_dataset(\n",
    "    root_dir=\"EnglishImg/EnglishImg/English/Img/GoodImg/Bmp\",\n",
    "    transform=transform,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"训练集大小: {len(train_loader.dataset)}\")\n",
    "print(f\"验证集大小: {len(val_loader.dataset)}\")\n",
    "print(f\"测试集大小: {len(test_loader.dataset)}\")\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)\n",
    "\n",
    "# 数据可视化\n",
    "to_img = T.ToPILImage()\n",
    "a = to_img(train_loader.dataset[0][0])  # size=[1, 28, 28]\n",
    "plt.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 获取类别数量\n",
    "label_num = len(full_dataset.class_to_idx)"
   ],
   "id": "e66bf08e90c3d02e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# # 获取展平后的训练集和验证集\n",
    "# X_train, y_train = flatten_images(train_loader)\n",
    "# X_val, y_val = flatten_images(val_loader)"
   ],
   "id": "f03dc6283d8519a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# # 训练SVM模型\n",
    "# svm_clf, train_acc, val_acc = train_svm(X_train, y_train, X_val, y_val)"
   ],
   "id": "b428cad1cad9b0b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 初始化模型\n",
    "cnn = mobilenet_v2(pretrained=True)\n",
    "print(cnn)\n",
    "\n",
    "# 检查 CUDA 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn.to(device)  # 将模型迁移到 GPU\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "# 定义损失函数\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "cnn = train_model(cnn, train_loader, val_loader, loss_func, optimizer, num_epochs=EPOCH)\n",
    "\n"
   ],
   "id": "9314fc69c8ac6597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80aeb1efe85ffd75"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
